{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "biological-union",
   "metadata": {},
   "source": [
    "# Task 2: Water\n",
    "---\n",
    "\n",
    "In this notebook, we are going to simulate a percolating substance that is able to propagate horizontally, as well as downwards.\n",
    "\n",
    "The aim of this notebook will be to determine the value of $q$ at which percolation switches from becoming more than 50\\% probable, to less than 50\\% probable.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Definition:</b> We will define the <b>percolation threshold</b> $q^*$ as the value of $q$ which corresponds to a percolation probability of $p = \\frac{1}{2}$.\n",
    "</div>\n",
    "\n",
    "### Motivation\n",
    "\n",
    "We might think of the percolating substance as water, and the simulation as representing water percolating through soil under gravity.\n",
    "\n",
    "This is actually a very relevant issue today; for example, understanding the way in which water percolates through different substances is crucial for accurate flood risk assessments, and flood mitigation strategies often aim to slow down the speed of percolation, without stopping it (which would lead to surface runoff and make the situation worse).\n",
    "\n",
    "It is also a very important process to understand in agriculture; undesirable weather conditions or poor management can easily cause soil to become highly compacted, preventing water from percolating down to the roots of the crop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-saskatchewan",
   "metadata": {},
   "source": [
    "## Running this variation of the model\n",
    "\n",
    "### Importing the code\n",
    "\n",
    "Of course, the first thing we need to do is import the relevant bits of code by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lattice import SquareLattice\n",
    "from model import PercolationModel\n",
    "from parameter_scan import parameter_scan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-thomas",
   "metadata": {},
   "source": [
    "### Setting up the model\n",
    "\n",
    "We're going to provide a second argument to `SquareLattice`, called `n_links`.\n",
    "In the previous notebook, `n_links` took its default value of 1, but we now want to change it to 3.\n",
    "\n",
    "Run the following cell to see the effect of changing `n_links`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(SquareLattice.n_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-morris",
   "metadata": {},
   "source": [
    "The way we will create a $50\\times 50$ lattice is\n",
    "```python\n",
    "lattice = SquareLattice(50, 50, n_links=3)\n",
    "```\n",
    "\n",
    "You might wonder why we don't just write\n",
    "```python\n",
    "lattice = SquareLattice(50, 50, 3)\n",
    "```\n",
    "Python allows both, but it is usually preferable to explicitly state the name of the argument which you are providing a value for.\n",
    "This is particularly helpful when a function has many arguments, and you don't want to rely on remembering the order in which they are supposed to appear.\n",
    "\n",
    "We could have been even more explicit and written\n",
    "```python\n",
    "lattice = SquareLattice(n_rows=50, n_cols=50, n_links=3)\n",
    "```\n",
    "but it's easy to remember that the first two arguments specify the size of the lattice.\n",
    "\n",
    "Run the cell below to produce an animation of this new variant of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 0.08\n",
    "\n",
    "lattice = SquareLattice(50, 50, n_links=3)\n",
    "model = PercolationModel(lattice, q)\n",
    "\n",
    "model.animate(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-affiliation",
   "metadata": {},
   "source": [
    "Notice that the percolation transition (when the water reaches the bottom row in the simulation) is surely going to be found at higher $q$ than in the previous notebook, since it doesn't only travel in one direction and hence it is able to move around an inert node.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning!</b> \n",
    "    You may also notice that the water 'wraps around' the box as it propagates.\n",
    "    This is a deliberate design choice, not something to worry about.\n",
    "    The reason for implementing these <b>periodic boundary conditions</b> is that they reduce undesirable 'boundary effects' from simulating a small system.\n",
    "</div>\n",
    "\n",
    "In the cell below, try a few different values of $q$, to get a rough idea of approximately where the percolation threshold lies (i.e. where the probability of percolating is $1/2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 0.3  # change me!\n",
    "\n",
    "lattice = SquareLattice(100, 50, n_links=3)\n",
    "model = PercolationModel(lattice, q)\n",
    "\n",
    "model.animate(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-mambo",
   "metadata": {},
   "source": [
    "## The percolation transition\n",
    "\n",
    "To estimate the percolation probability $p$, we will use a method similar to one you saw in the previous notebook.\n",
    "\n",
    "For each value of $q$ (the probability that a node is inert) we will run a number, $N$, of separate simulations and record the fraction of these that percolated,\n",
    "\n",
    "$$\n",
    "f = \\frac{1}{N} \\sum_{i=1}^N b_i \\tag{Estimator}\n",
    "$$\n",
    "\n",
    "where $b_i$ are equal to either 0 (for simulations which don't percolate) or 1 (for those that do), and $N$ is the number of simulations.\n",
    "\n",
    "The estimator for the standard error is the same as before:\n",
    "\n",
    "$$\n",
    "\\delta_f = \\sqrt{\\frac{f(1 - f)}{N-1}} \\tag{Standard error estimator}\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning!</b> \n",
    "    The error bars in the parameter scan plots below have an additional contribution, to account for the fact that the error on a point is not really 0 when $f = 0$ or $f = 1$, despite the formula for the standard error estimating saying that it is. You don't need to worry about this, but just be aware that we have made a correction.\n",
    "</div>\n",
    "\n",
    "In the example with light, we were able to *calculate* the true probability $p$, and compare this with our estimate.\n",
    "Here, we cannot do that (at least, I don't know how to calculate it...)\n",
    "\n",
    "Instead, we will be forced to draw conclusions based on the results of our simulations, and nothing else!\n",
    "This is why it was so valuable to check that everything was working properly in the previous notebook, when we knew the answer a priori."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-purple",
   "metadata": {},
   "source": [
    "## Estimating the percolation threshold using a 'fit'\n",
    "\n",
    "We want to find the percolation threshold, $q^*$, that gives a percolation probability of $p = 1/2$.\n",
    "To do this we will attempt to **fit** a function to the data, and use the **best-fit values of the parameters** of the function to estimate $q^*$.\n",
    "\n",
    "This may seem rather scary, but actually this approach might be familiar to you from school; you are probably quite familiar with a **line of best fit**, which corresponds to the function $f(x) = m x + c$, whose parameters are $m$ (the gradient) and $c$ (the y-intercept).\n",
    "The purpose of plotting a line of best fit is to extract the best-fit values of the gradient and the y-intercept, which might contain useful information about the system.\n",
    "This case is not different, other than that we will use a different function with different parameters.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Definition:</b> A <b>fit</b> is when you take a function and carefully tune the parameters so that the resulting line matches your data as well as possible.\n",
    "</div>\n",
    "\n",
    "Interested students can take 5 minutes at this point to have a read through the section [Further reading: Fitting functions to data](#Further-reading:-Fitting-functions-to-data) before returning to this point.\n",
    "\n",
    "The function we will attempt to fit to our data is a **Logistic** function\n",
    "\n",
    "$$\n",
    "S(q) = \\frac{1}{1 + e^{\\lambda (q - q_0)}}\n",
    "$$\n",
    "\n",
    "$S(q)$ is a very simple function that smoothly transitions between the asymptotes\n",
    "\n",
    "$$\n",
    "\\lim_{q\\to-\\infty} S(q) = 1 \\qquad\\qquad\n",
    "\\lim_{q\\to\\infty} S(q) = 0\n",
    "$$\n",
    "\n",
    "and has two *parameters* (just like the line of best fit):\n",
    "* The **mid-point** $q_0$, such that $S(q_0) = \\frac{1}{2}$. We will use this to *estimate* the percolation threshold, $q^*$.\n",
    "* The **steepness** $\\lambda$, which controls how quickly the transition from $p \\approx 1$ to $p \\approx 0$ occurs.\n",
    "\n",
    "The Logistic function is the simplest function that has essentially the correct shape \"backwards S\" shape to describe the percolation transition, and since we don't have any other clues (e.g. from an underlying theory), it is a good function to start with in this case.\n",
    "It is very important to realise that we do not have any fundamental reason to think $S(q)$ is going to fit the data perfectly, but we have to start somewhere!\n",
    "\n",
    "Run the cell below, which will measure $f$ over a range of $q$, plot the best-fit logistic function, and print the best-fit parameters.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning!</b> \n",
    "    Notice that the program has printed its estimates for $q_0$ and $\\lambda$ with a <b>very high precision</b>.\n",
    "    When it comes to reporting results in your lab book, you should decide on an appropriate number of significant figures for these values, using the rules you have learnt in the P1B course.\n",
    "    Hint: the full precision given by the program is <b>not appropriate</b>!\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "lattice = SquareLattice(50, 50, n_links=3)\n",
    "model = PercolationModel(lattice)\n",
    "\n",
    "qmin = 0\n",
    "qmax = 1\n",
    "N = 50\n",
    "\n",
    "parameter_scan(model, qmin, qmax, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-missile",
   "metadata": {},
   "source": [
    "What's wrong with this plot?\n",
    "\n",
    "Well, most of the data points are just zeroes or ones, and we have very few points that are close to the percolation transition at $f \\approx 0.5$.\n",
    "This is a problem because it reduces the accuracy of our determination of $q_0$, and also stops us from really being able to check whether a logistic curve fits the data well or not.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning!</b> \n",
    "    You may be wondering why the error printed by the program seems so small.\n",
    "    The answer is that the error is an <b>estimate</b> calculated using the data points themselves.\n",
    "    If there are a very small number of data points, it is more likely that the best-fit curve will be able to pass through all of their error bars, thus making the error appear small.\n",
    "    Don't be fooled!\n",
    "    If the error gets smaller as you remove data points, its probably a bad estimate.\n",
    "</div>\n",
    "\n",
    "In the cell below, try to find values for the end-points of the plot, `qmin` and `qmax`, so that most of the data points are not equal to zero or one.\n",
    "To speed things up, we have set the number of simulations per data point to be $N=25$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "lattice = SquareLattice(50, 50, n_links=3)\n",
    "model = PercolationModel(lattice)\n",
    "\n",
    "qmin = 0.4\n",
    "qmax = 0.43\n",
    "N = 25\n",
    "\n",
    "parameter_scan(model, qmin, qmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-rachel",
   "metadata": {},
   "source": [
    "## Residuals\n",
    "\n",
    "After fitting a function, there are a number of tests we can do to check whether or not the function fits the data well.\n",
    "This is particularly important since we really have no theoretical reason to believe that the data should follow a logistic curve.\n",
    "\n",
    "We will focus on two very simple but useful ways to check the quality of the fit:\n",
    "\n",
    "1. Checking that the best-fit curve passes through approximately 2/3 of the error bars.\n",
    "2. Checking for the absence of **structure in the residuals.**\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Definition:</b> <b>Residuals</b> are the differences between the data points and the best-fit curve, and can be plotted by subtracting the best-fit curve.\n",
    "    If the residuals show an underlying <b>structure</b> then this implies that the best-fit line is <b>systematically</b> incorrect.\n",
    "</div>\n",
    "\n",
    "Residuals do not contain any additional information that isn't already in the main plot, but by subtracting the best-fit curve it can be easier to check whether the fit is a good one or not.\n",
    "\n",
    "Run the cell below to see same examples of residuals with and without obvious underlying structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import residuals_examples\n",
    "residuals_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-creation",
   "metadata": {},
   "source": [
    "When the residuals have an obvious structure, as in the top two figures, this is telling us that our best-fit curve has not completely captured the underlying mechanism that produced the data.\n",
    "\n",
    "However, you should be careful not to over-interpret any small trends in the residuals.\n",
    "For example, if five points in a row are all below the best-fit curve, this doesn't necessarily represent underlying structure; it could easily be a statistical fluke.\n",
    "\n",
    "Run the cell below to perform another scan over the $q$ parameter. (This one will take longer than the previous ones - up to a minute or two!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "lattice = SquareLattice(100, 50, n_links=3)\n",
    "model = PercolationModel(lattice)\n",
    "\n",
    "qmin = 0.33\n",
    "qmax = 0.43\n",
    "N = 50\n",
    "\n",
    "parameter_scan(model, qmin, qmax, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-disorder",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Lab book: 2.1</b>\n",
    "Comment on the quality of the logistic best-fit curve, referring to the error bars and the residuals.\n",
    "    You should include a copy of the plot in your lab book; either copy and paste it into your lab book, or select `Save Image As` and save it to your device. <b>[3]</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-proposal",
   "metadata": {},
   "source": [
    "## Extrapolating to infinite lattice size\n",
    "\n",
    "There is one glaring issue with all the analysis we have done so far; all of our results depend on the size of the lattice!\n",
    "\n",
    "So-called **finite-volume effects** are an inevitability with simulations, since we cannot simulate an infinite system.\n",
    "However, it is possible to **extrapolate** results to the infinite limit, by looking at how the thing you're measuring varies as the size of simulations increases.\n",
    "\n",
    "The answers you get after extrapolating are more likely to agree with the physical world, which is generally much larger than any simulation we could do.\n",
    "For example, to simulate a $100m \\times 100m \\times 100m$ water reservoir at the level of individual channels (mm), we would need a simulation with $10^{15}$ nodes!\n",
    "\n",
    "To make things simpler (and quicker), we will leave the number of columns fixed at 50, and just vary the number of rows.\n",
    "Let's label the number of rows as $L$.\n",
    "We will then attempt to extract some infinite-lattice results for the percolation threshold, $\\lim_{L\\to\\infty} q^*$.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Table 2: </b> \n",
    "By varying `n_rows` ($L$), `qmin` and `qmax`, and repeatedly running the cell below, obtain estimates of $q^*$ for for 5-10 different lattice sizes ranging between `n_rows = 50` and `n_rows = 400`. \n",
    "Record your results in a table, making sure to note down the associated errors on $q_0$ <b>with an appropriate precision</b>.\n",
    "Also record your values for `qmin` and `qmax` in the table; these will need to be chosen for each lattice size so that almost all of the data points take values between 0 and 1, but are not 0 or 1. <b>[3]</b>\n",
    "</div>\n",
    "\n",
    "**Hint:** Don't go crazy fine-tuning `qmin` and `qmax`!\n",
    "The best way to go about this would be as follows:\n",
    "* Start from $L = 50$ and increase $L$ up to 400\n",
    "* For each successive value of $L$:\n",
    "  1. Run an initial simulation with a small number of points (e.g. `numq=10`), using `qmin` and `qmax` from the previous simulation with lower $L$.\n",
    "  2. Look at the resulting plot and note approximately where the graph reaches 0 and 1.\n",
    "  3. Set `qmin` to approximately where the graph reaches 1, and `qmax` to approximately where the graph reaches 0.\n",
    "\n",
    "**Hint:** You will find that `qmin` requires very little adjusting.\n",
    "\n",
    "We will also reduce the number of data points to 25, to speed things up.\n",
    "This is controlled by the `numq` parameter, which you should not change.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning!</b> \n",
    "    The larger simulations will take a while to complete. There is no need to generate more than 25 data points for each lattice size, and 25 repeats should also suffice. The largest lattice size may take up to 5 minutes, so grab a cup of coffee while you wait!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-exception",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 50  # L\n",
    "qmin = 0.3  # min value of q\n",
    "qmax = 0.5  # max value of q\n",
    "N = 25  # number of repeats\n",
    "numq = 25  # number of data points - no need to change!\n",
    "\n",
    "lattice = SquareLattice(n_rows, 50, n_links=3)\n",
    "model = PercolationModel(lattice)\n",
    "\n",
    "parameter_scan(model, qmin, qmax, N, numq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-loading",
   "metadata": {},
   "source": [
    "You may be able to tell, by just studying the numbers in your table, that the relationship between lattice size (`n_rows`) and the mid-point of the transition is not linear.\n",
    "\n",
    "We really can't say *a priori* what the relationship is, but it can be a idea to take logarithms to check if the relationship is a power-law $y \\propto x^a$.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Table 2 (part 2): </b> \n",
    "In the remaining three columns of your table, calculate $\\ln L$, $\\ln q_0$, and the error on $\\ln q_0$. <b>[2]</b>\n",
    "</div>\n",
    "\n",
    "This will require you to *propagate* the error on $q_0$ through the logarithm.\n",
    "Refer to the lab manual if you need to double check how to do this.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Plot 1:</b> \n",
    "Plot a graph of $\\ln q_0$ v.s. $\\ln L$ and draw a line of best fit.\n",
    "    Calculate the gradient of your line, with an estimate of its error. <b>[5]</b>\n",
    "</div>\n",
    "\n",
    "Now, let's say we take the lattice size to tend towards infinity (i.e. we move right-wards along the horizontal axis).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Lab book 2.4:</b>\n",
    "    It can be argued from a theoretical perspective that the percolation threshold $q^*$ will tend to zero as the number of rows tends to infinity: $\\lim_{L \\to \\infty} q^* = 0$.\n",
    "    Is your plot consistent with this hypothesis? <b>[1]</b>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Lab book 2.5:</b>\n",
    "    Can you explain why we might expect $\\lim_{L \\to \\infty} q^* = 0$ ? <b>[2]</b>\n",
    "</div>\n",
    "\n",
    "**Hint** for 2.5: Consider the probability that the water succeeds in percolating through a fixed number of rows, $l$ (remember, we keep the number of columns constant.)\n",
    "This will be a fixed number $0 < p_l < 1$ provided $q > 0$.\n",
    "Use the rule of probability $\\Pr (A \\cap B) = \\Pr (A)\\Pr(B)$ (discussed in the previous notebook) to write down the probability that the water percolates through $2l$ rows.\n",
    "Then, then extend this to the case of $kl$ rows where $k \\to \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-office",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Lab book 2.6:</b>\n",
    "    Suggest one thing that you would change in the model to make it a more realistic model of water percolating through soil under gravity. <b>[1]</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-visiting",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Notebook complete:</b> You're ready to move on to Notebook 3.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-efficiency",
   "metadata": {},
   "source": [
    "# Further reading: Fitting functions to data\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning!</b> \n",
    "    Instead of talking of fitting <b>functions</b> to data, it is more common to speak of fitting a <b>model</b> to some observations.\n",
    "    In this situation, a model is just a construct which, through the correct choice of its parameters, purports to be able to predict the observations.\n",
    "</div>\n",
    "\n",
    "Throughout your time at school or college you may have spent a good deal of time drawing straight lines through data on paper, or perhaps even using something like Excel.\n",
    "\n",
    "*Linear fits* are exceptionally useful, and as such, if it is possible to manipulate your data into a form where it is fit well by a straight line, then this is the approach you should take.\n",
    "As an example, we frequently take the logarithm of our variables if they obey a power law $y = A x^\\alpha$, since the variables $\\log y$ and $\\log x$ are related by the *linear* equation $\\log y = \\alpha \\log x + \\log A$, which is much easier to work with.\n",
    "\n",
    "However, it is not always possible to find a form in which our data is fit well by a straight line.\n",
    "We might imagine that there are more suitable functions that fit our data better.\n",
    "For example, if we were measuring the voltage of mains AC as a function of time, this is likely to be fit well by a sinusoid $V(t) = A \\sin\\big( (100\\pi~\\mathrm{s}^{-1}) t + \\varphi\\big)$, where we can tune the amplitude $A$ and the phase shift $\\varphi$ (the frequency we know is 50Hz) to get the best fit.\n",
    "In contrast, there is no way we can tune the gradient $m$ and the y-intercept $c$ to make the straight line $V(t) = m t + c$ fit the data well!\n",
    "\n",
    "But drawing a general function by hand is very difficult!\n",
    "How do we *actually do the fit*?\n",
    "This is a very complicated question, but all you need to understand at this point is that there are *algorithms* that *automatically* tune the *free parameters* of your function so that it fits the data as well as possible.\n",
    "\n",
    "The most important of these is called the **method of least squares**.\n",
    "We can write a computer algorithm that performs *least-squares fitting* by repeatedly 'nudging' the free parameters, so that each nudge results in a reduction in a quantity which encapsulates how well the function fits the data.\n",
    "For the curious, this quantity is result of adding up the *squared differences* between each data point and the function that you are trying to fit.\n",
    "This is the square of the residuals that we've been plotting.\n",
    "\n",
    "However, with such power comes, as always, great responsibility.\n",
    "The famous mathematician **John von Neumann** once said\n",
    "\n",
    "> \"With four free parameters I can fit an elephant, and with five I can make him wiggle his trunk!\"\n",
    "\n",
    "What he means is, you could come up with an extremely complicated function, with many many free parameters, and the least squares algorithm could twiddle these parameters so that the function passed through all of the error bars on your data points (which could be in the shape of an elephant!)\n",
    "This *does not mean that your function is a good representation of the underlying laws that generated the data*.\n",
    "\n",
    "If you're struggling to follow this, don't worry!\n",
    "Just bear it in mind as you continue your degree:\n",
    "**If two functions fit your data equally well, and one has fewer free parameters than the other, then you should choose the one with fewer parameters.**\n",
    "This is a version of **Occam's razor**, which basically says that \"the simplest theory that accurately predicts the phenomena is usually the correct one\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
